# BRPC技术略读

@(代码阅读)[RPC框架|原理]


##常见线程模型
1. 单线程reactor(epoll)
2. N：1线程库，一个用户态线程映射到一个系统线程，比如fiber（应该跟协程差不多）
2. 多线程reactor：有一个事件分配器将任务分给不同的线程，可能同时唤醒多个线程导致**惊群效应**
3. M:N线程库，M个用户线程（性能好，调度复杂）映射到N的系统线程中（性能差，调度简单）bthread，不是协程，由于协程，由于work stealling机制，一个bthread可以偷到其他系统线程

##原子指令
类似Java内存模型中的原子操作，可能存在以及共享资源竞争以及编译优化等问题，因此尽量避免共享资源，还有采用内存隔离（memory fencing）还有内存对齐导致的false sharing

###lock free和wait free
原子操作的指令是lock free和wait free的，所谓lock free指的是无论何时总有一个线程在干活，wait free指的是每个线程都在干事，注意用了锁的的方式一定不是lock free更不会是wait free，因为存在获得锁的线程crash导致没有线程能够获得锁，最后全部都挂起。一般通过CAS（compare and swap/set）机制实现

##IO操作
1. blocking IO：发起IO操作之后阻塞线程等待IO结束，同步模式
2. non-blocking IO：发起IO操作之后不阻塞，同时等待多个IO操作同时结束，是一种批量的同步，比如epoll等
3. 异步IO：用户传递一个回调等到IO结束后调用

当并发不高的情况下blocking还可以（无调度的优势），过高就要non-blocking

##DoubleBufferedData

负载均衡算法是一种读远多于写的数据结构,BRPC通过某种形式和读同步，但读之间没有竞争，通过读拿一把thread-local的锁，写需要拿到所有thread-local的锁，具体过程为：
1. 数据分前台后台
2. 读拿到自己线程所在的thread-local的锁，执行查询之后释放
3. 同时只有一个写线程，修改后台数据，切换前后台，挨个获得thread-local的锁并立刻释放，结束后修改老前台

读会挡住写，大部分读不存在竞争，获取所有thread-local锁是为了确保所有读线程都没有在访问老前台从而可以进行修改

##weight tree
权值根据反馈情况变化采用O(N)来进行负载均衡性能会比较差，采用完全二叉树来计算权值和修改将会达到log(N)

##base weight
QPS和延迟通过循环队列统计，给定一个大小为128的区间去统计数据，统计的思路可以简单的概括为QPS/latency即使节点性能很差也绝不会为0，除了删除节点将不会有权值为0的节点（避免饿死），

##inflight delay
对于统计中还未结束的RPC的统计方法为：当前时间-发出时间之和/未结束次数，这样比等超时要快多了

##一致性hash满足的条件

1. 平衡性：每个节点被选到的概率一样
2. 单调性：新的节点加入时，不会有请求在老节点之间移动，只会在新老之间移动
3. 分散性：当上游机器看到不同的下游列表时（不稳定的网络环境可能出现），请求尽量映射到少量的节点中
4. 负载：当上游机器看到不同的下游列表时，每台下游分到的请求数量尽量一样

##内存管理
1. 线程竞争少
2. 浪费空间少

采用类似SLAB机制的resourcepool，回收的对象并不立刻释放，只是可以被其他线程获取，创建机制如下
1. 如果thread-local 有free block则返回
2. 没有则尝试从全局取一个free block，取到的话到步骤一
3. 2不成功的话从全局取一个block，返回其中第一个对象

其中bthread为了创建的快速就是，根据偏移量和版本号来查看是否能获得对应bthread通过这种方式，实现重复使用

通过resourcepool分配bthread导致的一个问题是栈的大小固定，
##Timmer

posix系统的timer触发本质是系统调用，rpc发起之前设定一个timer，返回之后删除timer，因此大部分情况下timer只用一次

*libevent中通过小顶堆记录超时时间，如果中间没有其他事件阻止，弹出超时元素，调用相应回调函数，如此周而复始。所有timer相关的操作在一个线程完成，只要回调不阻塞就没问题，但是这不是多线程下的场景，多线程取对同一个time thread中的小顶堆操作会带来竞争和cache bouncing，而且这种timer还很有可能不会被触发



###多线程下的解决思路？
1. 使用一个timethread，所有的操作交给他来做
2. 创建的timer散列到多个bucket以降低线程间的竞争，有点像Java的concurrenthashmap,对于锁的粒度降级处理，在bucket内更新最近唤醒时间，再更新全局的
3. 删除修改标志，交由timethread删除
4. timethread每次回去检查bucket并且维护自己的小顶堆

**epoll_wait的精度较差，是毫秒






