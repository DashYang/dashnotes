#面试宝典
@(读书笔记)[求职]

##约瑟夫环
我们把N个人从0~n-1编号，可得序列
0,1,2,3,4,5...n-2，n-1
我们第一次拿走第k个人，则可以得到其编号为
(k-1) % n
然后我们把剩下的人，按照
k%n=0
(k+1)%n=1
...
（k+n-2)%n=n-2
编号，同理可以得到n=n-1的情况下,第一个拿走的人的编号如果为p，那么实际上他在n=n的情况下偏移为(k+p)%n
显然当n=1时最后一个拿走的编号为0，我们定义为
$ans_1=0$
那么根据上面的结果我们可以知道当n=2时他的编号应当偏移为
$ans_2=(ans_1+k)  \%2 $
依次可以推出
$ans_n=(ans_{n-1}+k) \% n$

##Trie树
在单词搜索的时候可以根据前缀补齐后面的，用在搜索引擎上，可以统计和排序大量字符串

##大数据查找统计问题
1. 分组hash，然后在组内比较重复，最后扫一遍取出重复最多的几个
2. 抽样过滤大部分数据
3. 精髓在于内存中不能放下所有数据

##反射机制

Java反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。反射机制获取类的三种方法
```java
	//1 直接通过类名.Class的方式得到  
    clazz = Person.class;  
    System.out.println("通过类名: " + clazz);  
    //2 通过对象的getClass()方法获取,这个使用的少（一般是传的是Object，不知道是什么类型的时候才用）  
    Object obj = new Person();  
    clazz = obj.getClass();  
    System.out.println("通过getClass(): " + clazz);  
  
    //3 通过全类名获取，用的比较多，但可能抛出ClassNotFoundException异常  
    clazz = Class.forName("com.java.reflection.Person");  
    System.out.println("通过全类名获取: " + clazz);  
	//4. invoke调用方法
	Object obj = clazz.newInstance();  
    method2.invoke(obj, "changwen", 22);  
    //5、获取指定对象的Field的值  
    Object val = field.get(person);  
    //6. 获取注解  
    Annotation annotation = method.getAnnotation(AgeValidator.class);  
```

##控制反转和依赖注入

控制反转指将对象的创建交给特定的容器来做，依赖注入是容器可以自动的根据需要创建相应的类，同一个概念的不同表述

##消息队列
系统解耦：项目开始时，无法确定最终需求，不同进程间，添加一层，实现解耦，方便今后的扩展。生产者和消费者专注自己的工作即可。
消息缓存：系统中，不同进程处理消息速度不同，MQ，可以实现不同Process之间的缓冲，即，写入MQ的速度可以尽可能地快，而处理消息的速度可以适当调整（或快、或慢）。

##Kafka(消息队列)
主要由三部分组成，生产者、消费者和集群（包括多个分区），其中集群和消费者需要通过zookeeper交互
![kafka structure](https://images2017.cnblogs.com/blog/409917/201708/409917-20170808000013143-1162263091.png)

1. 高效性：消息集合压缩，还有备份机制。能够支持上千个客户端同时读写，kafka集群支持热扩展
2. 容错性：一个topic能有非常多个副本，如果服务器配置足够好，可以配很多个，副本的个数决定了有多少个broker存放写入的数据；简单的来说副本是以partition为单位的，存放副本也可以这样简单的理解，备份若干个partition、但是只能有一个partition被选为Leader用于读写。partition（分区）数量设置最好大于consumer数量，其实，这样设计的思想就是保证每个消费者都有一个partition。生产者在向kafka集群发送消息的时候，可以通过指定分区来发送到指定的分区中也可以通过指定均衡策略来将消息发送到不同的分区中如果不指定，就会采用默认的随机均衡策略，将消息随机的存储到不同的分区中，允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）
3. 持久性：消息持久化到本地
4. 高并发：支持数千个客户端同时读写

###设计思想
**Kakfa Broker Leader**的选举：Kakfa Broker集群受Zookeeper管理。所有的Kafka Broker节点一起去Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower。（这个过程叫Controller在ZooKeeper注册Watch）。这个Controller会监听其他的Kafka Broker的所有信息，如果这个kafka broker controller宕机了，在zookeeper上面的那个临时节点就会消失，此时所有的kafka broker又会一起去Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower。例如：一旦有一个broker宕机了，这个kafka broker controller会读取该宕机broker上所有的partition在zookeeper上的状态，并选取ISR列表中的一个replica作为partition leader（如果ISR列表中的replica全挂，选一个幸存的replica作为leader; 如果该partition的所有的replica都宕机了，则将新的leader设置为-1，等待恢复，等待ISR中的任一个Replica“活”过来，并且选它作为Leader；或选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader），这个broker宕机的事情，kafka controller也会通知zookeeper，zookeeper就会通知其他的kafka broker。

**Consumergroup**：各个consumer（consumer 线程）可以组成一个组（Consumer group ），partition中的每个message只能被组（Consumer group ）中的一个consumer（consumer 线程）消费，如果一个message可以被多个consumer（consumer 线程）消费的话，那么这些consumer必须在不同的组。Kafka不支持一个partition中的message由两个或两个以上的同一个consumer group下的consumer thread来处理，除非再启动一个新的consumer group。所以如果想同时对一个topic做消费的话，启动多个consumer group就可以了，但是要注意的是，这里的多个consumer的消费都必须是顺序读取partition里面的message，新启动的consumer默认从partition队列最头端最新的地方开始阻塞的读message。

如果producer的流量增大，当前的topic的parition数量=consumer数量，这时候的应对方式就是很想扩展：增加topic下的partition，同时增加这个consumer group下的consumer。

kafka在性能上严重依赖文件系统的本身特性

除磁盘IO之外,我们还需要考虑网络IO,这直接关系到kafka的吞吐量问题.kafka并没有提供太多高超的技巧;对于producer端,可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker;对于consumer端也是一样,批量fetch多条消息

###使用场景
- 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。
- 消息系统：解耦和生产者和消费者、缓存消息等。
- 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。
- 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。
- 流式处理：比如spark streaming和storm
- 事件源

##SPRING原理 

1.IOC控制反转

概念：控制权由对象本身转向容器，由容器根据配置文件创建对象实例并实现各个对象的依赖关系。

核心：bean工厂


2.AOP面向切面编程

a.静态代理
根据每个具体类分别编写代理类
根据一个接口编写一个代理类
b.动态代理JDK和cglib，DK动态代理与CGLib动态代理均是实现Spring AOP的基础，JDK方式需要基于统一接口
CGLIB包的底层是通过使用一个小而快的字节码处理框架ASM(Java字节码操控框架)，来转换字节码并生成新的类。他其实就像JVM一样，可以加载一个指定的类。这样我们就可以实现在运行时期生成我们自定义的class了
针对一个方面编写一个InvocationHandler，然后借用JDK反射包中的Proxy类为各种接口动态生成相应的代理类，代理可以添加相应的拦截类在调用之前或者之后进行操作
JDK动态代理的原理是根据定义好的规则，用传入的接口创建一个新类，这就是为什么采用动态代理时为什么只能用接口引用指向代理，而不能用传入的类引用执行动态类。
CGLib采用的是用创建一个继承实现类的子类，用asm库动态修改子类的代码来实现的，所以可以用传入的类引用执行代理类
CGLib创建的动态代理对象性能比JDK创建的动态代理对象的性能高不少，但是CGLib在创建代理对象时所花费的时间却比JDK多得多，所以对于单例的对象，因为无需频繁创建对象，用CGLib合适，反之，使用JDK方式要更为合适一些。同时，由于CGLib由于是采用动态创建子类的方法，对于final方法，无法进行代理

RMI是远程代理
    
##go语言特性

1. 自动垃圾回收
2. 更丰富的内置类型）（数组切片、类似vector）
3. 支持函数多返回值
4. 错误处理（defer关键字语句的含义是不管程序是否出现异常，均在函数退出时自动执行相关代码。）
5. 匿名函数和闭包（函数名可以作为参数传递）
6. 类型和接口（接口interface和类型可以直接转换）
7. 并发编程（Go语言引入了goroutine概念，它使得并发编程变得非常简单。通过使用goroutine而不是裸用操作系统的并发机制，以及使用消息传递来共享内存而不是使用共享内存来通信，Go语言让并发编程变得更加轻盈和安全。通过在函数调用前使用关键字go，我们即可让该函数以goroutine方式执行，goroutine是一种比线程更加轻盈、更省资源的协程。进程之间只能通过一对通信原语实现协作。Go语言用channel（通道）这个概念来轻巧地实现了CSP模型。channel的使用方式比较接近Unix系统中的管道（pipe）概念，可以方便地进行跨goroutine的通信。由于一个进程内创建的所有goroutine运行在同一个内存地址空间中，因此如果不同的goroutine不得不去访问共享的内存变量，访问前应该先获取相应的读写锁。Go语言标准库中的sync包提供了完备的读写锁功能）
8. 支持反射（encoding/json、encoding/xml、encoding/gob、encoding/binary）
9. 和c的混编很方便

Go语言通过系统的线程来多路派遣这些函数的执行，使得每个用go关键字执行的函数可以运行成为一个单位协程。当一个协程阻塞的时候，调度器就会自动把其他协程安排到另外的线程中去执行，从而实现了程序无等待并行化运行。而且调度的开销非常小，一颗CPU调度的规模不下于每秒百万次，这使得我们能够创建大量的goroutine，从而可以很轻松地编写高并发程序，达到我们想要的目的。协程的调度不依赖内核

##Mybatis原理

![enter image description here](http://img.blog.csdn.net/20141028140852531?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHVhbmxvdWlz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##异常机制

1. try catch中只要有finally语句都要执行（有特例：如果try 或 catch 里面有 exit（0）就不会执行finally了）；
2. finally语句在try或catch中的return语句执行之后返回之前执行，且finally里的修改语句不能影响try或catch中 return已经确定的返回值； 若finally里也有return语句则覆盖try或catch中的return语句直接返回；
3. 在遵守第（2）条return的情况下，执行顺序是：try-->catch（如果有异常的话）-->finally；

##类的创建顺序

父类静态代码块-->子类静态代码块-->父类普通代码块-->父类构造方法-->子类代码块-->子类构造方法；

##线程启动顺序

start()方法启动线程将自动调用 run()方法，启动线程的是start

##Mysql
###文件存储类型Innodb和Mysiam
MyISAM管理非事务表，提供高速存储和检索，以及全文搜索能力。
MyISAM是Mysql的默认存储引擎。当create创建新表时，未指定新表的存储引擎时，默认使用MySIAM。每个MySIAM在磁盘上存储成三个文件。文件名都和表名相同，扩展名分别是.frm（存储表定义）、.MYD (MYData，存储数据)、.MYI (MYIndex，存储索引)。数据文件和索引文件可以放置在不同的目录，平均分布io，获得更快的速度。 没有存储限制，不支持支持事务，表锁，性能较高，
InnoDB存储引擎用于事务处理应用程序，具有众多特性，包括ACID事务支持，提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM存储引擎，InnoDB写的处理效率差一些并且会占用更多的磁盘空间以保留数据和索引。有存储限制，支持事务，行锁，支持外键
与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等。
独享表空间存储方式使用.ibd文件，并且每个表一个ibd文件
共享表空间存储方式使用.ibdata文件，所有表共同使用一个ibdata文件

###索引
B-树的特性：
1. 关键字集合分布在整颗树中；
2. 任何一个关键字出现且只出现在一个结点中；
3. 搜索有可能在非叶子结点结束；
4. 其搜索性能等价于在关键字全集内做一次二分查找；
5. 自动层次控制；

B+树是B-树的变体，也是一种多路搜索树：
1. 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的；
2. 不可能在非叶子结点命中；
3. 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层；
4. 更适合文件索引系统；

mysql中普遍使用B+Tree做索引，但在实现上又根据聚簇索引和非聚簇索引而不同。
聚簇索引
所谓聚簇索引，就是指主索引文件和数据文件为同一份文件，聚簇索引主要用在Innodb存储引擎中。在该索引实现方式中B+Tree的叶子节点上的data就是数据本身，key为主键，如果是一般索引的话，data便会指向对应的主索引.
在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。
非聚簇索
非聚簇索引就是指B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。主要用在MyISAM存储引擎中
 非聚簇索引比聚簇索引多了一次读取数据的IO操作，所以查找性能上会差。

###数据库自增主键
为了防止重复只能在一台实例上请求下一个ID，造成性能瓶颈，每次需要一个就生成一个也会导致IO压力较大，解决方案
1. 多个数据库设置不同的步长
2. 一次取多个


美团点评解决方案[LEAF](https://tech.meituan.com/MT_Leaf.html)，通过多个LEAF服务去数据库获得自增序列，再提供给请求自增id的服务，数据库是单点，但是设置主从复制或者paxos确保宕机之后的发号

###Redis
Redis set的时候可能出现问题（跟多线程的一样，实际操作的对象和看到的对象不一致），最简单的可以用incr，decr。比较复杂一点的可以用锁，或者乐观锁，先Watch变量，如果变量不改则set，否则不set，也就是Java里面的CAS机制

###Java线程池
假设一个服务器完成一项任务所需时间为：T1 创建线程时间，T2 在线程中执行任务的时间，T3 销毁线程时间。 如果：T1 + T3 远大于 T2，则可以采用线程池，以提高服务器性能。
一个线程池包括以下四个基本组成部分：
1. 线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务；
2. 工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务；
3. 任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等；
4. 任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。

好处
1. 节约进程创建和销毁时间
2. 通过调整线程数量充分利用计算机资源


 线程池的工作过程如下：
1. 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。
2. 当调用 execute() 方法添加一个任务时，线程池会做如下判断：
3. 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务；
4. 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列；
5. 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务；
6. 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常RejectExecutionException。
7. 当一个线程完成任务时，它会从队列中取下一个任务来执行。
8. 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。

SingleThreadExecutor：单个后台线程，管理所有工作线程，添加和移除，异常退出会有一个新的顶替
FixedThreadPool：只有核心线程的线程池,大小固定。创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。
CachedThreadPool：无界线程池，可以进行自动线程回收。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小
ScheduledThreadPool：核心线程池固定，大小无限的线程池。此线程池支持定时以及周期性执行任务的需求，创建一个周期性执行任务的线程池。如果闲置,非核心线程池会在DEFAULT_KEEPALIVEMILLIS时间内回收。


###JUC（Java并发工具包）

1. volatile 关键字
2. 原子变量AtomicInteger
3. CAS 算法，乐观锁，如果跟预期的不一样则不更新
4. 并发容器类（1）ConcurrentHashMap 同步容器类是 Java5 增加的一个线程安全的哈希表;介于 HashMap 与 Hashtable 之间;内部采用"锁分段"机制替代Hashtable的独占锁,进而提高性能
5. 线程池
6. 线程ScheduledThreadPool
7. 锁机制(ReentrantLock, ReadWriteLockDemo)
8. fork&join大任务分解最后合并

###dubbo
![enter image description here](http://images2015.cnblogs.com/blog/524341/201604/524341-20160414132910191-1796519559.jpg)
服务容器负责启动，加载，运行服务提供者。
服务提供者在启动时，向注册中心注册自己提供的服务。
服务消费者在启动时，向注册中心订阅自己所需的服务。
注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心

###接口幂等
1. 全局唯一ID，记录每一次操作序列防止重复
2. 去重表，利用数据库唯一索引插入失败引发回滚
3. 多版本控制，在操作中增加版本号
4. 状态机控制：有几种状态变换


###热部署
监控特定文件夹下的可执行文件，发生改变的时候启动类加载器重新加载